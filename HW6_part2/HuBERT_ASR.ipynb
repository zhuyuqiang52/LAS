{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDywzhcPwF0R"
   },
   "source": [
    "# mini-project: Automatic Speech Recognition(2/2)\n",
    "We split the mini-project into two notebooks. \n",
    "\n",
    "This is ***2/2*** of the mini-project. Make sure to also complete and submit ***LAS_ASR.ipynb***.\n",
    "\n",
    "***Credits***: ELENE 6820, Xilin Jiang(xj2289 at columbia) and Prof. Nima Mesgarani.\n",
    "\n",
    "Rewritten from Yi Luo's ASR homework in 2021.\n",
    "\n",
    "I acknowledge SpeechBrain [Tutorials](https://speechbrain.github.io/tutorial_basics.html) for reference and ChatGPT for rephrasing and testing.\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TiUkm3YRxvAF"
   },
   "source": [
    "Hope you spend a good time with LAS. \n",
    "\n",
    "In this notebook, we will be conducting experiments on a self-supervised learning model to assess its ability to recognize words. \n",
    "\n",
    "We will revisit **HuBERT** model which is briefly mentioned in Lecture 6 Speech Signal Representation.\n",
    "\n",
    "You don't need to re-download the dataset. But if you are using Colab, you need to re-mount the folder from Google Drive and re-install SpeechBrain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aUCrmvQVN5MQ",
    "outputId": "d068cedf-06b4-44c8-d9f8-63013b2c3d64"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')\n",
    "# %cd drive/MyDrive/HW6_part2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p6yPMFZ0OJc5"
   },
   "outputs": [],
   "source": [
    "# !pip install speechbrain\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a3osNlBgNeEy",
    "outputId": "69dad439-348b-43de-85c9-c1a7160262da"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import librosa\n",
    "import datetime\n",
    "import speechbrain as sb\n",
    "import matplotlib.pyplot as plt\n",
    "from hyperpyyaml import load_hyperpyyaml\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LdW4mJts1EdH"
   },
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SiyqKQh7y_Fd"
   },
   "source": [
    "Load hyperparameters and set the experiment time stamp. \n",
    "\n",
    "***You need to reload*** `hparams/hubert.yaml` everytime you change hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2uZerojjNeEz",
    "outputId": "40d6eb61-75c9-414e-c15b-0eda4a1a6fc5"
   },
   "outputs": [],
   "source": [
    "HPARAM_FILE = 'hparams/hubert.yaml'\n",
    "\n",
    "time_stamp = datetime.datetime.now().strftime('%Y-%m-%d+%H-%M-%S')\n",
    "print(f'Experiment Time Stamp: {time_stamp}')\n",
    "argv = [HPARAM_FILE, '--time_stamp', time_stamp]\n",
    "\n",
    "hparam_file, run_opts, overrides = sb.parse_arguments(argv)\n",
    "\n",
    "with open(HPARAM_FILE) as f:\n",
    "    hparams = load_hyperpyyaml(f, overrides)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QV0xcAjj0uoy"
   },
   "source": [
    "## 2. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W16FQNZVuaNQ"
   },
   "source": [
    "We will use HuBERT as our encoder for speech recognization . Other speech SSL models can be used in a similar way.\n",
    "\n",
    "Please read the paper: https://arxiv.org/abs/2106.07447 and the lecture slides for how HuBERT learns speech representations in a self-supervised manner.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGIfzEjX5fgc"
   },
   "source": [
    "We can get a pretrained HuBERT model from [Transformers](https://huggingface.co/docs/transformers/index) library by ðŸ¤— Hugging Face. \n",
    "\n",
    "ðŸ¤— Hugging Face is the 'github' for ML models. You can upload and download models just like codes.\n",
    "\n",
    "\n",
    "The [checkpoint](https://huggingface.co/facebook/hubert-large-ls960-ft) we will be using is first pretrained on 60,000-hour [Libri-Light](https://arxiv.org/abs/1912.07875) dataset without supervision and then finetuned on 960-hour LibriSpeech for ASR. In this notebook, we will build an ASR system from HuBERT embeddings, rather than from MFCC or shallow CNN encodings.\n",
    "\n",
    "***Note***: In this homework, you are ***not*** pretraining or finetuning HuBERT. The HuBERT checkpoint parameters are already optimized and should be fixed. You only need to implement and optimize the missing part in the ASR pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nlxrJUuuNeE1",
    "outputId": "90844e53-2f3a-4923-d507-454da0e97947"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import HubertModel\n",
    "\n",
    "# Initialize pretrained HuBERT. \n",
    "# Ignore the warning 'Some weights of the model checkpoint ... were not used'\n",
    "# We will not use the pretrained language model.\n",
    "encoder = HubertModel.from_pretrained(hparams['hubert_path']).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-cHbGnQ96sI"
   },
   "source": [
    "Let's encode a sample audio with HuBERT. Our HuBERT encoder has [24](https://huggingface.co/facebook/hubert-large-ls960-ft/blob/main/config.json#L66) hidden layers, so there are 25 levels of encoded features we can use. We will observe different performance if we choose the embeddings from different levels. By default, HuBERT only returns the features of the last layer. Set `output_hidden_states=True` to get them all.\n",
    "\n",
    "Pay attention to HuBERT feature dimension. Can you see anything meaningful from HuBERT features? (Probably not.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RJYGzcVWNeE2"
   },
   "outputs": [],
   "source": [
    "sr = hparams['sample_rate']\n",
    "sample_audio_path = 'data/LibriSpeech/train-clean-5/1088/134315/1088-134315-0000.flac'\n",
    "sample_audio = librosa.load(sample_audio_path, sr=sr)[0]\n",
    "sample_audio = torch.tensor(sample_audio).unsqueeze(dim=0).to(device)\n",
    "\n",
    "from IPython.display import Audio\n",
    "Audio(sample_audio.cpu(), rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    hubert_feats = encoder(sample_audio, output_hidden_states=True).hidden_states\n",
    "    hubert_last_feats = hubert_feats[-1]\n",
    "    hubert_middle_feats = hubert_feats[12]\n",
    "    \n",
    "hubert_dim = hubert_last_feats.shape[-1] \n",
    "    \n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(121)\n",
    "plt.imshow(hubert_last_feats[0].cpu().detach().numpy().T)\n",
    "plt.xlabel('T')\n",
    "plt.title('HuBERT last-layer features')\n",
    "plt.subplot(122)\n",
    "plt.imshow(hubert_middle_feats[0].cpu().detach().numpy().T)\n",
    "plt.xlabel('T')\n",
    "plt.title('HuBERT 12th-layer features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sn4HSrLR4X2C"
   },
   "source": [
    "Next, we load the tokenizer for HuBERT. The vocabulary set contains 32 characters(26 English letters + 6 special symbols). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kAVo9UQgNeE4",
    "outputId": "9c7b8eea-7c78-4130-a04b-86168a510bab"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(hparams['hubert_path'])\n",
    "print(tokenizer.get_vocab())\n",
    "vocab_size = len(tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sypkmj36AxUY"
   },
   "source": [
    "## What's Next?\n",
    "\n",
    "Recall the task of Speech Recognition. We need to build a system to map a waveform to words.\n",
    "\n",
    "Right now, we have an encoder mapping from waveform to HuBERT features $\\mathbb{R}^{1 \\times T} â†’ \\mathbb{R}^{C \\times N}$, where $C$ is the feature dimension and $N$ is the number of frames, and a tokenizer to decode words given token indices of all frames $\\{0, 1, ..., V\\}^{N}, V=32$.\n",
    "\n",
    "We have the frontend and the backend. The only missing part in the middle is to predict the most likely character from the HuBERT features. \n",
    "\n",
    "You need to implement a ***linear*** CTC `Predictor` $\\mathbb{R}^{C \\times N} â†’ \\mathbb{R}^{V \\times N}$ to estimate token probabilities. \n",
    "\n",
    "I wrote a `Decoder` which takes the argmax of your prediction.\n",
    "\n",
    "**TODD(8/10)**. Implement `Predictor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BU2qopERNeE3"
   },
   "outputs": [],
   "source": [
    "class Predictor(nn.Module):\n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        super(Predictor, self).__init__()\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, preds):\n",
    "        '''\n",
    "        preds (B, N, V)\n",
    "        '''\n",
    "        ids = torch.argmax(preds, dim=-1)\n",
    "        return ids\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WdQOtFch1Ysc"
   },
   "source": [
    "## 3. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2jc4lVR3Bix"
   },
   "source": [
    "We encode words with a new tokenizer in the `text_pipeline`. The rest of the data pipeline is unchanged.\n",
    "\n",
    "We use the full `train-clean-5` folder for training and the first 100 utterances from `dev-clean-2` and `test-clean` for validation and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yxi_saaJNeE4"
   },
   "outputs": [],
   "source": [
    "bos_index = hparams['bos_index']\n",
    "eos_index = hparams['eos_index']\n",
    "@sb.utils.data_pipeline.takes('words')\n",
    "@sb.utils.data_pipeline.provides('words', 'tokens_list', 'tokens_bos', 'tokens_eos', 'tokens', 'attention_mask')\n",
    "def text_pipeline(words):\n",
    "    yield words\n",
    "    encoded = tokenizer(words)\n",
    "    tokens_list, attention_mask = encoded['input_ids'], encoded['attention_mask']\n",
    "    yield tokens_list\n",
    "    tokens_bos = torch.LongTensor([bos_index] + (tokens_list))\n",
    "    yield tokens_bos\n",
    "    # we use same eos and bos indexes as in pretrained model\n",
    "    tokens_eos = torch.LongTensor(tokens_list + [eos_index])\n",
    "    yield tokens_eos\n",
    "    tokens = torch.LongTensor(tokens_list)\n",
    "    yield tokens\n",
    "    attention_mask = torch.LongTensor(attention_mask)\n",
    "    yield attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lwQSHzYzNeE6",
    "outputId": "41b405ec-4825-40e2-aa12-05bd78412606"
   },
   "outputs": [],
   "source": [
    "from speechbrain.dataio.dataset import DynamicItemDataset\n",
    "\n",
    "# Create Train Dataset\n",
    "train_manifest_path = hparams['train_manifest_path']\n",
    "print(f'Creating training set from {train_manifest_path}')\n",
    "train_set = DynamicItemDataset.from_json(train_manifest_path)\n",
    "train_set.add_dynamic_item(\n",
    "    sb.dataio.dataio.read_audio, takes='file_path', provides='signal'\n",
    ")\n",
    "train_set = train_set.filtered_sorted(sort_key='length', select_n=hparams['n_train'])\n",
    "train_set.add_dynamic_item(text_pipeline)\n",
    "train_set.set_output_keys(\n",
    "    ['id', 'signal', 'words', 'tokens_list', 'tokens_bos', 'tokens_eos', 'tokens', 'attention_mask']\n",
    ")\n",
    "    \n",
    "# Create Valid Dataset\n",
    "valid_manifest_path = hparams['valid_manifest_path']\n",
    "print(f'Creating validation set from {valid_manifest_path}')\n",
    "valid_set = DynamicItemDataset.from_json(valid_manifest_path)\n",
    "valid_set.add_dynamic_item(\n",
    "    sb.dataio.dataio.read_audio, takes='file_path', provides='signal'\n",
    ")\n",
    "valid_set = valid_set.filtered_sorted(sort_key='length', select_n=hparams['n_valid'])\n",
    "valid_set.add_dynamic_item(text_pipeline)\n",
    "valid_set.set_output_keys(\n",
    "    ['id', 'signal', 'words', 'tokens_list', 'tokens_bos', 'tokens_eos', 'tokens', 'attention_mask']\n",
    ")\n",
    "\n",
    "# Create Test Dataset \n",
    "test_manifest_path = hparams['test_manifest_path'] \n",
    "print(f'Creating validation set from {test_manifest_path}')\n",
    "test_set = DynamicItemDataset.from_json(test_manifest_path)\n",
    "test_set.add_dynamic_item(\n",
    "    sb.dataio.dataio.read_audio, takes='file_path', provides='signal'\n",
    ")\n",
    "test_set = test_set.filtered_sorted(sort_key='length', select_n=hparams['n_test'])\n",
    "test_set.add_dynamic_item(text_pipeline)\n",
    "test_set.set_output_keys(\n",
    "    ['id', 'signal', 'words', 'tokens_list', 'tokens_bos', 'tokens_eos', 'tokens', 'attention_mask']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5CBrJ_oB06Nt"
   },
   "source": [
    "## 4. Brain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pyxsykwHtRtc"
   },
   "source": [
    "**TODD(9/10)**. Complete `compute_forward` function.\n",
    "\n",
    "*Hint1*: Use the last-layer embedding of HuBERT. You can set `output_hidden_states=False` or `hidden_states[-1]`.\n",
    "\n",
    "*Hint2*: Don't update HuBERT. Wrap HuBERT encoding `with torch.no_grad()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DUUoVFczNeE8"
   },
   "outputs": [],
   "source": [
    "class HubertASR(sb.core.Brain):\n",
    "    \n",
    "    def on_fit_start(self):\n",
    "        super().on_fit_start()\n",
    "        self.modules.encoder.feature_extractor._freeze_parameters()\n",
    "        self.checkpointer.add_recoverable('predictor', self.modules.predictor)\n",
    "\n",
    "    def compute_forward(self, batch, stage):\n",
    "        \"\"\"Forward computations from the waveform batches to the output probabilities.\"\"\"\n",
    "        batch = batch.to(self.device)\n",
    "        wavs, wav_lens = batch.signal\n",
    "                    \n",
    "        # HuBERT encode features\n",
    "        feats = None\n",
    "        \n",
    "        # Estimate tokens\n",
    "        preds = None\n",
    "        raise NotImplementedError\n",
    "\n",
    "        # Compute outputs\n",
    "        if stage != sb.Stage.TRAIN:\n",
    "            hyps = self.modules.decoder(preds.detach())\n",
    "        else:\n",
    "            hyps = None\n",
    "\n",
    "        return p_ctc, wav_lens, hyps\n",
    "\n",
    "    def compute_objectives(self, predictions, batch, stage):\n",
    "        \"\"\"Computes the loss (CTC) given predictions and targets.\"\"\"\n",
    "\n",
    "        (p_ctc, wav_lens, hyps,) = predictions\n",
    "\n",
    "        ids = batch.id\n",
    "        tokens_eos, tokens_eos_lens = batch.tokens_eos\n",
    "        tokens, tokens_lens = batch.tokens\n",
    "        attention_mask, _ = batch.attention_mask\n",
    "        tokens = tokens.masked_fill(attention_mask.ne(1), -100)\n",
    "        loss_ctc = self.hparams.ctc_cost(\n",
    "            p_ctc, tokens, wav_lens, tokens_lens\n",
    "        ).mean()\n",
    "        loss = loss_ctc\n",
    "\n",
    "        if stage != sb.Stage.TRAIN:\n",
    "            # Decode token terms to words\n",
    "            predicted_words = [\n",
    "                self.tokenizer.decode(utt_seq).split(\" \") \n",
    "                for utt_seq in hyps\n",
    "            ]\n",
    "            target_words = [wrd.split(\" \") for wrd in batch.words]\n",
    "            self.wer_metric.append(ids, predicted_words, target_words)\n",
    "            self.cer_metric.append(ids, predicted_words, target_words)\n",
    "            \n",
    "        return loss\n",
    "\n",
    "    def evaluate_batch(self, batch, stage):\n",
    "        \"\"\"Computations needed for validation/test batches\"\"\"\n",
    "        with torch.no_grad():\n",
    "            predictions = self.compute_forward(batch, stage=stage)\n",
    "            loss = self.compute_objectives(predictions, batch, stage=stage)\n",
    "        return loss.detach()\n",
    "\n",
    "    def on_stage_start(self, stage, epoch):\n",
    "        \"\"\"Gets called at the beginning of each epoch\"\"\"\n",
    "        if stage != sb.Stage.TRAIN:\n",
    "            self.wer_metric = self.hparams.wer_computer()\n",
    "            self.cer_metric = self.hparams.cer_computer()\n",
    "\n",
    "    def on_stage_end(self, stage, stage_loss, epoch):\n",
    "        \"\"\"Gets called at the end of a epoch.\"\"\"\n",
    "        # Compute/store important stats\n",
    "        stage_stats = {'loss': stage_loss}\n",
    "        if stage == sb.Stage.TRAIN:\n",
    "            self.train_stats = stage_stats\n",
    "        else:\n",
    "            stage_stats['WER'] = self.wer_metric.summarize('error_rate')\n",
    "            stage_stats[\"CER\"] = self.cer_metric.summarize(\"error_rate\")\n",
    "\n",
    "        if stage == sb.Stage.VALID:\n",
    "            self.checkpointer.save_and_keep_only(\n",
    "                meta={'WER': stage_stats['WER'], 'epoch': epoch},\n",
    "                min_keys=['WER'],\n",
    "            )\n",
    "\n",
    "        elif stage == sb.Stage.TEST:\n",
    "            with open(self.hparams.wer_file, 'w') as w:\n",
    "                self.wer_metric.write_stats(w)\n",
    "                \n",
    "        print(f'Epoch {epoch}: ', stage, stage_stats) \n",
    "\n",
    "    def fit_batch(self, batch):\n",
    "        outputs = self.compute_forward(batch, sb.Stage.TRAIN)\n",
    "        loss = self.compute_objectives(outputs, batch, sb.Stage.TRAIN)\n",
    "        loss.backward()\n",
    "        if self.check_gradients(loss):\n",
    "            self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "        self.optimizer_step += 1\n",
    "        \n",
    "        return loss.detach().cpu()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqgSsfke1h5A"
   },
   "source": [
    "## 5. Experiments\n",
    "Instantiate `HubertASR` with our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ae-jJqbfNeE-"
   },
   "outputs": [],
   "source": [
    "# Add encoder, predictor and decoder\n",
    "modules = {\n",
    "    'encoder': encoder,\n",
    "    'predictor': Predictor(hubert_dim, vocab_size),\n",
    "    'decoder': Decoder()\n",
    "}\n",
    "\n",
    "brain = HubertASR(\n",
    "    modules, \n",
    "    hparams=hparams, \n",
    "    opt_class=hparams['opt_class'],\n",
    "    run_opts=run_opts,\n",
    "    checkpointer=hparams['checkpointer']\n",
    ")\n",
    "\n",
    "brain.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FiyPJVuYQbO5"
   },
   "source": [
    "**TODD(10/10)**. Train and evaluate the model.\n",
    "\n",
    "You need to score a ***WER $\\leq 3$*** on the testing set for full points.\n",
    "\n",
    "Your training loss may increase within an epoch, because utterances are sorted by length and longer utterances produce larger loss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "8SNLnsjjNeE_",
    "outputId": "941db1df-db26-491a-fb3e-e9013a1cfafb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "brain.fit(\n",
    "    epoch_counter=hparams['epoch_counter'],\n",
    "    train_set=train_set,\n",
    "    valid_set=valid_set,\n",
    "    train_loader_kwargs=hparams['train_dataloader_opts'],\n",
    "    valid_loader_kwargs=hparams['valid_dataloader_opts'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6t4ePAIvNeFA"
   },
   "outputs": [],
   "source": [
    "brain.hparams.wer_file = os.path.join(\n",
    "    hparams['save_folder'], 'wer.txt'\n",
    ")\n",
    "brain.evaluate(\n",
    "    test_set,\n",
    "    test_loader_kwargs=hparams['test_dataloader_opts'],\n",
    "    min_key='WER'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "xj-dev",
   "language": "python",
   "name": "xj-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
